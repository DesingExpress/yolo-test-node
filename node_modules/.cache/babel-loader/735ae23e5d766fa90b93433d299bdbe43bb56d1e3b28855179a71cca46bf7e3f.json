{"ast":null,"code":"import * as tf from \"@tensorflow/tfjs\";\nimport { renderBoxes } from \"./renderBox\";\nimport labels from \"./labels.json\";\nconst numClass = labels.length;\n\n/**\n * Preprocess image / frame before forwarded into the model\n * @param {HTMLVideoElement|HTMLImageElement} source\n * @param {Number} modelWidth\n * @param {Number} modelHeight\n * @returns input tensor, xRatio and yRatio\n */\nconst preprocess = (source, modelWidth, modelHeight) => {\n  let xRatio, yRatio; // ratios for boxes\n\n  const input = tf.tidy(() => {\n    const img = tf.browser.fromPixels(source);\n\n    // padding image to square => [n, m] to [n, n], n > m\n    const [h, w] = img.shape.slice(0, 2); // get source width and height\n    const maxSize = Math.max(w, h); // get max size\n    const imgPadded = img.pad([[0, maxSize - h],\n    // padding y [bottom only]\n    [0, maxSize - w],\n    // padding x [right only]\n    [0, 0]]);\n    xRatio = maxSize / w; // update xRatio\n    yRatio = maxSize / h; // update yRatio\n\n    return tf.image.resizeBilinear(imgPadded, [modelWidth, modelHeight]) // resize frame\n    .div(255.0) // normalize\n    .expandDims(0); // add batch\n  });\n\n  return [input, xRatio, yRatio];\n};\n\n/**\n * Function run inference and do detection from source.\n * @param {HTMLImageElement|HTMLVideoElement} source\n * @param {tf.GraphModel} model loaded YOLOv8 tensorflow.js model\n * @param {HTMLCanvasElement} canvasRef canvas reference\n * @param {VoidFunction} callback function to run after detection process\n */\nexport const detect = async (source, model, canvasRef, callback = () => {}) => {\n  const [modelWidth, modelHeight] = model.inputShape.slice(1, 3); // get model width and height\n\n  tf.engine().startScope(); // start scoping tf engine\n  const [input, xRatio, yRatio] = preprocess(source, modelWidth, modelHeight); // preprocess image\n\n  const res = model.net.execute(input); // inference model\n  const transRes = res.transpose([0, 2, 1]); // transpose result [b, det, n] => [b, n, det]\n  const boxes = tf.tidy(() => {\n    const w = transRes.slice([0, 0, 2], [-1, -1, 1]); // get width\n    const h = transRes.slice([0, 0, 3], [-1, -1, 1]); // get height\n    const x1 = tf.sub(transRes.slice([0, 0, 0], [-1, -1, 1]), tf.div(w, 2)); // x1\n    const y1 = tf.sub(transRes.slice([0, 0, 1], [-1, -1, 1]), tf.div(h, 2)); // y1\n    return tf.concat([y1, x1, tf.add(y1, h),\n    //y2\n    tf.add(x1, w) //x2\n    ], 2).squeeze();\n  }); // process boxes [y1, x1, y2, x2]\n\n  const [scores, classes] = tf.tidy(() => {\n    // class scores\n    const rawScores = transRes.slice([0, 0, 4], [-1, -1, numClass]).squeeze(0); // #6 only squeeze axis 0 to handle only 1 class models\n    return [rawScores.max(1), rawScores.argMax(1)];\n  }); // get max scores and classes index\n\n  const nms = await tf.image.nonMaxSuppressionAsync(boxes, scores, 500, 0.45, 0.2); // NMS to filter boxes\n\n  const boxes_data = boxes.gather(nms, 0).dataSync(); // indexing boxes by nms index\n  const scores_data = scores.gather(nms, 0).dataSync(); // indexing scores by nms index\n  const classes_data = classes.gather(nms, 0).dataSync(); // indexing classes by nms index\n\n  renderBoxes(canvasRef, boxes_data, scores_data, classes_data, [xRatio, yRatio]); // render boxes\n  tf.dispose([res, transRes, boxes, scores, classes, nms]); // clear memory\n\n  callback();\n  tf.engine().endScope(); // end of scoping\n};\n\n/**\n * Function to detect video from every source.\n * @param {HTMLVideoElement} vidSource video source\n * @param {tf.GraphModel} model loaded YOLOv8 tensorflow.js model\n * @param {HTMLCanvasElement} canvasRef canvas reference\n */\nexport const detectVideo = (vidSource, model, canvasRef) => {\n  /**\n   * Function to detect every frame from video\n   */\n  const detectFrame = async () => {\n    if (vidSource.videoWidth === 0 && vidSource.srcObject === null) {\n      const ctx = canvasRef.getContext(\"2d\");\n      ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height); // clean canvas\n      return; // handle if source is closed\n    }\n\n    detect(vidSource, model, canvasRef, () => {\n      window.requestAnimationFrame(detectFrame); // get another frame\n    });\n  };\n\n  detectFrame(); // initialize to detect every frame\n};","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}