{"ast":null,"code":"import _regeneratorRuntime from \"/Users/kim-euntae/Documents/Dev/yolov8-nxv/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _asyncToGenerator from \"/Users/kim-euntae/Documents/Dev/yolov8-nxv/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _slicedToArray from \"/Users/kim-euntae/Documents/Dev/yolov8-nxv/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";\nimport * as tf from \"@tensorflow/tfjs\";\nimport { renderBoxes } from \"./renderBox\";\nimport labels from \"./labels.json\";\nvar numClass = labels.length;\n\n/**\n * Preprocess image / frame before forwarded into the model\n * @param {HTMLVideoElement|HTMLImageElement} source\n * @param {Number} modelWidth\n * @param {Number} modelHeight\n * @returns input tensor, xRatio and yRatio\n */\nvar preprocess = function preprocess(source, modelWidth, modelHeight) {\n  var xRatio, yRatio; // ratios for boxes\n\n  var input = tf.tidy(function () {\n    var img = tf.browser.fromPixels(source);\n\n    // padding image to square => [n, m] to [n, n], n > m\n    var _img$shape$slice = img.shape.slice(0, 2),\n      _img$shape$slice2 = _slicedToArray(_img$shape$slice, 2),\n      h = _img$shape$slice2[0],\n      w = _img$shape$slice2[1]; // get source width and height\n    var maxSize = Math.max(w, h); // get max size\n    var imgPadded = img.pad([[0, maxSize - h],\n    // padding y [bottom only]\n    [0, maxSize - w],\n    // padding x [right only]\n    [0, 0]]);\n    xRatio = maxSize / w; // update xRatio\n    yRatio = maxSize / h; // update yRatio\n\n    return tf.image.resizeBilinear(imgPadded, [modelWidth, modelHeight]) // resize frame\n    .div(255.0) // normalize\n    .expandDims(0); // add batch\n  });\n\n  return [input, xRatio, yRatio];\n};\n\n/**\n * Function run inference and do detection from source.\n * @param {HTMLImageElement|HTMLVideoElement} source\n * @param {tf.GraphModel} model loaded YOLOv8 tensorflow.js model\n * @param {HTMLCanvasElement} canvasRef canvas reference\n * @param {VoidFunction} callback function to run after detection process\n */\nexport var detect = /*#__PURE__*/function () {\n  var _ref = /*#__PURE__*/_asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee(source, model, canvasRef) {\n    var callback,\n      _model$inputShape$sli,\n      _model$inputShape$sli2,\n      modelWidth,\n      modelHeight,\n      _preprocess,\n      _preprocess2,\n      input,\n      xRatio,\n      yRatio,\n      res,\n      transRes,\n      boxes,\n      _tf$tidy,\n      _tf$tidy2,\n      scores,\n      classes,\n      nms,\n      boxes_data,\n      scores_data,\n      classes_data,\n      _args = arguments;\n    return _regeneratorRuntime().wrap(function _callee$(_context) {\n      while (1) switch (_context.prev = _context.next) {\n        case 0:\n          callback = _args.length > 3 && _args[3] !== undefined ? _args[3] : function () {};\n          _model$inputShape$sli = model.inputShape.slice(1, 3), _model$inputShape$sli2 = _slicedToArray(_model$inputShape$sli, 2), modelWidth = _model$inputShape$sli2[0], modelHeight = _model$inputShape$sli2[1]; // get model width and height\n          tf.engine().startScope(); // start scoping tf engine\n          _preprocess = preprocess(source, modelWidth, modelHeight), _preprocess2 = _slicedToArray(_preprocess, 3), input = _preprocess2[0], xRatio = _preprocess2[1], yRatio = _preprocess2[2]; // preprocess image\n          res = model.net.execute(input); // inference model\n          transRes = res.transpose([0, 2, 1]); // transpose result [b, det, n] => [b, n, det]\n          boxes = tf.tidy(function () {\n            var w = transRes.slice([0, 0, 2], [-1, -1, 1]); // get width\n            var h = transRes.slice([0, 0, 3], [-1, -1, 1]); // get height\n            var x1 = tf.sub(transRes.slice([0, 0, 0], [-1, -1, 1]), tf.div(w, 2)); // x1\n            var y1 = tf.sub(transRes.slice([0, 0, 1], [-1, -1, 1]), tf.div(h, 2)); // y1\n            return tf.concat([y1, x1, tf.add(y1, h),\n            //y2\n            tf.add(x1, w) //x2\n            ], 2).squeeze();\n          }); // process boxes [y1, x1, y2, x2]\n          _tf$tidy = tf.tidy(function () {\n            // class scores\n            var rawScores = transRes.slice([0, 0, 4], [-1, -1, numClass]).squeeze(0); // #6 only squeeze axis 0 to handle only 1 class models\n            return [rawScores.max(1), rawScores.argMax(1)];\n          }), _tf$tidy2 = _slicedToArray(_tf$tidy, 2), scores = _tf$tidy2[0], classes = _tf$tidy2[1]; // get max scores and classes index\n          _context.next = 10;\n          return tf.image.nonMaxSuppressionAsync(boxes, scores, 500, 0.45, 0.2);\n        case 10:\n          nms = _context.sent;\n          // NMS to filter boxes\n          boxes_data = boxes.gather(nms, 0).dataSync(); // indexing boxes by nms index\n          scores_data = scores.gather(nms, 0).dataSync(); // indexing scores by nms index\n          classes_data = classes.gather(nms, 0).dataSync(); // indexing classes by nms index\n          renderBoxes(canvasRef, boxes_data, scores_data, classes_data, [xRatio, yRatio]); // render boxes\n          tf.dispose([res, transRes, boxes, scores, classes, nms]); // clear memory\n\n          callback();\n          tf.engine().endScope(); // end of scoping\n        case 18:\n        case \"end\":\n          return _context.stop();\n      }\n    }, _callee);\n  }));\n  return function detect(_x, _x2, _x3) {\n    return _ref.apply(this, arguments);\n  };\n}();\n\n/**\n * Function to detect video from every source.\n * @param {HTMLVideoElement} vidSource video source\n * @param {tf.GraphModel} model loaded YOLOv8 tensorflow.js model\n * @param {HTMLCanvasElement} canvasRef canvas reference\n */\nexport var detectVideo = function detectVideo(vidSource, model, canvasRef) {\n  /**\n   * Function to detect every frame from video\n   */\n  var detectFrame = /*#__PURE__*/function () {\n    var _ref2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2() {\n      var ctx;\n      return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n        while (1) switch (_context2.prev = _context2.next) {\n          case 0:\n            if (!(vidSource.videoWidth === 0 && vidSource.srcObject === null)) {\n              _context2.next = 4;\n              break;\n            }\n            ctx = canvasRef.getContext(\"2d\");\n            ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height); // clean canvas\n            return _context2.abrupt(\"return\");\n          case 4:\n            detect(vidSource, model, canvasRef, function () {\n              window.requestAnimationFrame(detectFrame); // get another frame\n            });\n          case 5:\n          case \"end\":\n            return _context2.stop();\n        }\n      }, _callee2);\n    }));\n    return function detectFrame() {\n      return _ref2.apply(this, arguments);\n    };\n  }();\n  detectFrame(); // initialize to detect every frame\n};","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}